{"cells":[{"cell_type":"markdown","metadata":{"id":"XQSzNovNpM1J"},"source":["# Env Setting"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":1085,"status":"ok","timestamp":1700943988166,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"q-7c4UHkyo9q"},"outputs":[],"source":["model_saved_name=\"model_colab.ckpt\"\n","dataset_path=\"data/std_dataset\"\n"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"P71KxLw8ElbE","executionInfo":{"status":"ok","timestamp":1700943988666,"user_tz":0,"elapsed":9,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"}},"outputId":"135f7908-7eef-4e12-ff18-1429f17e439b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/DeepLearning/MagNet_Single_comb/MagNet_comb_3E6_cycle'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":47}],"source":["import os\n","current_path = os.getcwd()\n","index = current_path.find(\"DeepLearning\")\n","current_path = current_path[index:]\n","current_path='/content/drive/MyDrive/'+current_path\n","current_path=current_path.replace('\\\\','/')\n","\"colab_dir = \"+current_path\n","current_path"]},{"cell_type":"markdown","metadata":{"id":"9oHNo5pjyo9r"},"source":["## Colab"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1700943988666,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"BY47VVFEpe5q"},"outputs":[],"source":["colab_dir = '/content/drive/MyDrive/DeepLearning/MagNet_Single_comb/MagNet_comb_3E6_cycle'  # example for colab\n","\n","platform = 'auto' # auto detect platform (colab, windows_local, linux_local, unknown)\n","#platform = 'colab'\n","#platform = 'windows_local'\n","#platform = 'linux_local'\n","#platform = 'unknown'"]},{"cell_type":"markdown","metadata":{"id":"F3jSpmi4pU5n"},"source":["### Path config"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3022,"status":"ok","timestamp":1700943991681,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"7EAuEhFiocKq","outputId":"8d2fb630-1a45-4ee0-8713-fb78f6fd8bdc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","current execution path:  /content/drive/MyDrive/DeepLearning/MagNet_Single_comb/MagNet_comb_3E6_cycle\n","\n","current platform:  colab\n"]}],"source":["import os\n","\n","try:\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","except ImportError:\n","    if os.path.exists('c:/'):  # check if it is windows\n","        platform = 'windows_local'\n","    elif os.path.exists('/home/'):  # check if it is linux\n","        platform = 'linux_local'\n","    else:\n","        platform = 'unknown'\n","else:\n","    platform = 'colab'\n","\n","if platform == 'colab':\n","  os.chdir(colab_dir)\n","\n","print('\\ncurrent execution path: ', os.getcwd())  #获取当前工作目录路径\n","print('\\ncurrent platform: ', platform)  #获取当前工作目录路径"]},{"cell_type":"markdown","metadata":{"id":"CHDhat43ogMQ"},"source":["# Cuda check"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1700943991681,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"7Aipe8RLOzQk","outputId":"d3e93bf2-e550-41f5-dc95-ac6acafd2689"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda good!\n","GPU num:  1\n","GPU type:  NVIDIA A100-SXM4-40GB\n","GPU memory: 42.48 Gbyte\n"]}],"source":["import torch\n","\n","gpu_num = 0\n","cuda_ready = False\n","\n","if torch.cuda.is_available():\n","    cuda_ready = True\n","    print('cuda good!')\n","    gpu_num = torch.cuda.device_count()\n","    if (gpu_num < 1):\n","        print('GPU unavailable')\n","    else:\n","        print('GPU num: ', gpu_num)  # 查看GPU数量\n","        for gpu in range(gpu_num):\n","            print('GPU type: ', torch.cuda.get_device_name(gpu))  # 查看GPU名称\n","            print('GPU memory: {:.2f} Gbyte'.format(\n","                torch.cuda.get_device_properties(gpu).total_memory /\n","                1e9))  # 查看GPU总内存\n","else:\n","    cuda_ready = False\n","    print('cuda unavailable!')\n"]},{"cell_type":"markdown","metadata":{"id":"9z1whOUtaPbq"},"source":["# Start coding"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1700943991681,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"LSndganfafmO","outputId":"c0731ce7-327a-4e9b-ef87-1ec497e9a123"},"outputs":[{"output_type":"stream","name":"stdout","text":["colab\n","/content/drive/MyDrive/DeepLearning/MagNet_Single_comb/MagNet_comb_3E6_cycle\n","True\n","/content/drive/MyDrive/DeepLearning/MagNet_Single_comb/MagNet_comb_3E6_cycle\n"]}],"source":["print(platform)\n","print(os.getcwd())\n","print(cuda_ready)\n","print(os.path.abspath(''))"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1700943991681,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"8q3WkMaUtvSZ"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","\n","\n","import NW_LSTM\n","import NN_DataLoader"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1700943991681,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"ftzXEac3tyHc","outputId":"18dcad36-432f-4e71-fc76-54d62eca1c60"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device using  cuda\n","LSTMSeq2One(\n","  (lstm): LSTM(1, 30, num_layers=3, batch_first=True)\n","  (fc1): Linear(in_features=32, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=196, bias=True)\n","  (fc3): Linear(in_features=196, out_features=128, bias=True)\n","  (fc4): Linear(in_features=128, out_features=96, bias=True)\n","  (fc5): Linear(in_features=96, out_features=32, bias=True)\n","  (fc6): Linear(in_features=32, out_features=32, bias=True)\n","  (fc7): Linear(in_features=32, out_features=16, bias=True)\n","  (fc8): Linear(in_features=16, out_features=1, bias=True)\n","  (relu): ReLU()\n","  (leaky_relu): LeakyReLU(negative_slope=0.01)\n","  (elu): ELU(alpha=1.0)\n","  (sigmoid): Sigmoid()\n",")\n","Total number of parameters:  90653\n","Pre-train model loaded\n"]}],"source":["# Check if CUDA is available and if so, set the device to GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#device = torch.device(\"cpu\")\n","print(\"Device using \",device)\n","\n","# Instantiate the model with appropriate dimensions\n","model = model = NW_LSTM.get_global_model().to(device)\n","\n","# Print the model architecture and parameters number\n","print(model)\n","print(\"Total number of parameters: \", sum(p.numel() for p in model.parameters()))\n","\n","# Load the pre-train model if it exists\n","try:\n","    model.load_state_dict(torch.load(model_saved_name))\n","    print(\"Pre-train model loaded\")\n","except:\n","    print(\"No model found, start training from scratch\")\n","    pass\n","\n","# Define the loss function and optimizer\n","#loss_fn = nn.MSELoss()\n","loss_fn = NW_LSTM.RelativeLoss()\n","#loss_fn = NW_LSTM.RelativeLoss_abs()\n","optimizer = optim.AdamW(model.parameters(), lr=2e-4)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=0, last_epoch=-1)"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":139014,"status":"ok","timestamp":1700944130680,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"X6OqVZExt06B","outputId":"05b263ca-cc85-403d-d496-5da03c2cdb0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 10/700, Training Loss: 2.062e-05, Remain time: 2.2 min\n","Epoch 20/700, Training Loss: 5.032e-05, Remain time: 2.2 min\n","Epoch 30/700, Training Loss: 1.551e-04, Remain time: 2.3 min\n","Epoch 40/700, Training Loss: 4.253e-05, Remain time: 2.3 min\n","Epoch 50/700, Training Loss: 1.424e-05, Remain time: 2.2 min\n","Epoch 60/700, Training Loss: 1.945e-05, Remain time: 2.1 min\n","Epoch 70/700, Training Loss: 1.918e-05, Remain time: 2.1 min\n","Epoch 80/700, Training Loss: 1.258e-05, Remain time: 2.0 min\n","Epoch 90/700, Training Loss: 1.510e-05, Remain time: 2.0 min\n","Epoch 100/700, Training Loss: 1.740e-05, Remain time: 2.1 min\n","Epoch 110/700, Training Loss: 1.991e-05, Remain time: 1.9 min\n","Epoch 120/700, Training Loss: 2.211e-05, Remain time: 2.0 min\n","Epoch 130/700, Training Loss: 1.974e-05, Remain time: 1.9 min\n","Epoch 140/700, Training Loss: 3.281e-05, Remain time: 2.0 min\n","Epoch 150/700, Training Loss: 1.891e-05, Remain time: 1.8 min\n","Epoch 160/700, Training Loss: 1.671e-04, Remain time: 1.8 min\n","Epoch 170/700, Training Loss: 2.705e-05, Remain time: 1.7 min\n","Epoch 180/700, Training Loss: 3.304e-05, Remain time: 1.7 min\n","Epoch 190/700, Training Loss: 2.338e-05, Remain time: 1.7 min\n","Epoch 200/700, Training Loss: 1.236e-04, Remain time: 1.6 min\n","Epoch 210/700, Training Loss: 3.504e-05, Remain time: 1.6 min\n","Epoch 220/700, Training Loss: 2.843e-05, Remain time: 1.6 min\n","Epoch 230/700, Training Loss: 3.284e-05, Remain time: 1.5 min\n","Epoch 240/700, Training Loss: 2.861e-05, Remain time: 1.5 min\n","Epoch 250/700, Training Loss: 1.849e-05, Remain time: 1.5 min\n","Epoch 260/700, Training Loss: 1.739e-05, Remain time: 1.5 min\n","Epoch 270/700, Training Loss: 2.349e-05, Remain time: 1.4 min\n","Epoch 280/700, Training Loss: 1.639e-05, Remain time: 1.4 min\n","Epoch 290/700, Training Loss: 1.208e-05, Remain time: 1.3 min\n","Epoch 300/700, Training Loss: 2.839e-05, Remain time: 1.4 min\n","Epoch 310/700, Training Loss: 1.441e-05, Remain time: 1.3 min\n","Epoch 320/700, Training Loss: 1.716e-05, Remain time: 1.3 min\n","Epoch 330/700, Training Loss: 1.583e-05, Remain time: 1.2 min\n","Epoch 340/700, Training Loss: 4.377e-05, Remain time: 1.2 min\n","Epoch 350/700, Training Loss: 3.430e-05, Remain time: 1.1 min\n","Epoch 360/700, Training Loss: 6.035e-05, Remain time: 1.1 min\n","Epoch 370/700, Training Loss: 1.040e-04, Remain time: 1.1 min\n","Epoch 380/700, Training Loss: 1.746e-05, Remain time: 1.1 min\n","Epoch 390/700, Training Loss: 5.870e-05, Remain time: 1.0 min\n","Epoch 400/700, Training Loss: 2.718e-04, Remain time: 1.0 min\n","Epoch 410/700, Training Loss: 3.004e-05, Remain time: 1.0 min\n","Epoch 420/700, Training Loss: 1.666e-05, Remain time: 0.9 min\n","Epoch 430/700, Training Loss: 1.720e-05, Remain time: 0.9 min\n","Epoch 440/700, Training Loss: 2.376e-05, Remain time: 0.9 min\n","Epoch 450/700, Training Loss: 1.729e-05, Remain time: 0.8 min\n","Epoch 460/700, Training Loss: 2.263e-05, Remain time: 0.8 min\n","Epoch 470/700, Training Loss: 2.047e-05, Remain time: 0.8 min\n","Epoch 480/700, Training Loss: 1.499e-05, Remain time: 0.7 min\n","Epoch 490/700, Training Loss: 1.686e-05, Remain time: 0.7 min\n","Epoch 500/700, Training Loss: 1.395e-05, Remain time: 0.7 min\n","Epoch 510/700, Training Loss: 1.829e-05, Remain time: 0.6 min\n","Epoch 520/700, Training Loss: 1.079e-05, Remain time: 0.6 min\n","Epoch 530/700, Training Loss: 1.734e-05, Remain time: 0.6 min\n","Epoch 540/700, Training Loss: 1.873e-05, Remain time: 0.5 min\n","Epoch 550/700, Training Loss: 4.821e-04, Remain time: 0.5 min\n","Epoch 560/700, Training Loss: 4.556e-05, Remain time: 0.5 min\n","Epoch 570/700, Training Loss: 5.124e-05, Remain time: 0.4 min\n","Epoch 580/700, Training Loss: 2.117e-04, Remain time: 0.4 min\n","Epoch 590/700, Training Loss: 1.133e-04, Remain time: 0.4 min\n","Epoch 600/700, Training Loss: 2.640e-05, Remain time: 0.3 min\n","Epoch 610/700, Training Loss: 2.351e-05, Remain time: 0.3 min\n","Epoch 620/700, Training Loss: 1.951e-05, Remain time: 0.3 min\n","Epoch 630/700, Training Loss: 3.445e-05, Remain time: 0.2 min\n","Epoch 640/700, Training Loss: 2.180e-05, Remain time: 0.2 min\n","Epoch 650/700, Training Loss: 1.443e-05, Remain time: 0.2 min\n","Epoch 660/700, Training Loss: 5.116e-05, Remain time: 0.1 min\n","  Model saved , Validation Loss: 1.694e-04, lr: 5.742e-05\n","Epoch 670/700, Training Loss: 1.906e-05, Remain time: 0.1 min\n","  Model saved , Validation Loss: 1.690e-04, lr: 3.626e-05\n","  Model saved , Validation Loss: 1.686e-04, lr: 2.929e-05\n","Epoch 680/700, Training Loss: 1.044e-05, Remain time: 0.1 min\n","  Model saved , Validation Loss: 1.683e-04, lr: 1.557e-05\n","Epoch 690/700, Training Loss: 2.333e-05, Remain time: 0.0 min\n","  Model saved , Validation Loss: 1.683e-04, lr: 4.894e-06\n","Epoch 700/700, Training Loss: 1.667e-05, Remain time: 0.0 min\n"]}],"source":["# Default para in desktop env\n","epochs = 10\n","valid_batch_size=1000\n","\n","if platform == \"colab\":\n","  epochs = 700\n","  valid_batch_size=2000\n","\n","\n","train_dataloader = NN_DataLoader.get_dataLoader(os.path.normpath(dataset_path +\n","                                                            \"/train.mat\"),\n","                                          batch_size=128)\n","\n","# Get validation data\n","valid_dataloader = NN_DataLoader.get_dataLoader(os.path.normpath(dataset_path +\n","                                                            \"/valid.mat\"),\n","                                            batch_size=valid_batch_size)\n","valid_inputs, valid_targets = next(iter(valid_dataloader))\n","valid_inputs, valid_targets = valid_inputs.to(device), valid_targets.to(device)\n","\n","\n","\n","# estimate time used for training\n","import time\n","t0 = time.perf_counter()\n","\n","# Save the model with the lowest validation loss\n","with torch.no_grad():\n","    valid_outputs = model(valid_inputs)\n","    # Compute loss\n","    minium_loss = loss_fn(valid_outputs, valid_targets)\n","\n","# Train the model\n","for epoch in range(epochs):\n","\n","    # estimate time used for one epoch(s)\n","    t_epoch = time.perf_counter() - t0\n","    t0 = time.perf_counter()\n","\n","    # Train one epoch\n","    for i, (train_inputs, train_targets) in enumerate(train_dataloader):\n","        # Move data to device\n","        train_inputs, train_targets = train_inputs.to(device), train_targets.to(device)\n","\n","        # Forward pass\n","        train_outputs = model(train_inputs)\n","\n","        # Compute loss\n","        loss = loss_fn(train_outputs, train_targets)\n","\n","        # Backward pass and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Compute validation loss\n","    if epoch > 0:\n","        with torch.no_grad():\n","            valid_outputs = model(valid_inputs)\n","            # Compute loss\n","            valid_loss = loss_fn(valid_outputs, valid_targets)\n","\n","        if valid_loss < minium_loss:\n","            minium_loss = valid_loss\n","            torch.save(model.state_dict(), model_saved_name)\n","            print(f\"  Model saved , Validation Loss: {valid_loss.item():.3e}, lr: {optimizer.param_groups[0]['lr']:.3e}\")\n","\n","    # update lr\n","    scheduler.step()\n","\n","\n","    # Print loss every 10 epochs\n","    if (epoch + 1) % 10 == 0:\n","        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {loss.item():.3e}, \"\n","            #   f\"Validation Loss: {valid_loss.item():.3e} ,\"\n","              f\"Remain time: {t_epoch/60 * (epochs - epoch - 1):.1f} min\")\n"]},{"cell_type":"markdown","metadata":{"id":"Cszxdb72g9AO"},"source":["## GPU monitor\n","### nvidia-smi -l 3"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["CHDhat43ogMQ"],"gpuType":"A100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"markdown","metadata":{},"source":["This notebook is used to verify the accuracy of the models trained in the *batch_training.ipynb* stage. \n","\n","For each material in the directory, this loss is inferred using a batch of previously unseen data partitioned as training data in the pre processing stage. This predicted loss is then compared to the laboratory measured loss for each datapoint to calculate the spread of errors for each material loss model. This distribution is then measured and plotted and saved to validate the accuracy of the trained model. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import torch\n","import os\n","import pandas as pd\n","\n","import NW_LSTM\n","import Maglib\n","import linear_std\n","import MagNet"]},{"cell_type":"markdown","metadata":{},"source":["Configure working data directory containing at least the 'Processed Training Data' and 'Trained Weights folders."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_dir=r'C:\\Users\\ossia\\Documents\\GitHub\\MagLearn-Bristol-2\\Single Pipeline\\preprocessed_training_dataset_4'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["weights_dir = os.path.join(data_dir, 'Trained Weights') # Directory containing weighted material models\n","training_data_dir = os.path.join(data_dir, 'Processed Training Data') # Directory of pre-processed training data\n","validation_dir = os.path.join(data_dir, 'Validation') # Directory where plots and files for model validation will be stored\n","os.makedirs(validation_dir, exist_ok=True) # Creates folder if it does not already exist\n","\n","# Check if 'Trained Weights' folder exists and contains material models\n","if os.path.isdir(weights_dir):\n","    # Get the list of models from the weights_dir folder, extracting the name of each .ckpt model which conventionally corresponds to the material name\n","    weights = [os.path.splitext(item)[0] for item in os.listdir(weights_dir)\n","                if item.endswith('.ckpt') and os.path.isfile(os.path.join(weights_dir, item))]\n","    if weights == []:\n","        raise RuntimeError(f'No .ckpt models found in \"{weights_dir}\", ensure \"Trained Weights\" subfolder in data_dir contains individual .ckpt files corresponding to each trained material.')\n","else:\n","    print(f'No subfolder labeled \"Trained Weights\" found at \"{data_dir}\", please ensure this exists and contains individual .ckpt files corresponding to each trained material.')\n","\n","# Print the list of trained models found\n","print(\"Identified weightings for the following materials:\", weights)"]},{"cell_type":"markdown","metadata":{},"source":["**Device Selection**\n","\n","Ensure pytorch compute device set to CPU. GPU overhead makes it slower for these small batch sizes. If working with significantly larger batch sizes, defined by *max_samples* later in notebook, it may be faster to use GPU acceleration."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"elapsed":7189,"status":"error","timestamp":1691832850627,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":-60},"id":"Gz-wXmweGAMO","outputId":"b7d0e8d8-c98e-4909-ce20-f170a8ddaefa"},"outputs":[],"source":["device = torch.device(\"cpu\")\n","print(\"Device set to\", device)"]},{"cell_type":"markdown","metadata":{},"source":["**Validation Loop**\n","\n","*max_samples* limits number of timeseries-temperature-frequency-loss samples per material, increase if validating larger datasets and compute time is not limited\n","\n","Loop through all material models in directory: loading test dataset, loading weights, running model inferrence, comparing inferred loss with actual loss, plotting results. Then summary csv for all models is saves to same 'Validation' sub-directory."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1691832850628,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":-60},"id":"2l-kYSquGAMR"},"outputs":[],"source":["max_samples = 2000\n","\n","error_summary = []\n","data = []\n","\n","for material in weights:\n","    magData = Maglib.MagLoader(os.path.join(training_data_dir, material, 'test.mat'))\n","\n","    # Instantiate the model with appropriate dimensions\n","    model = NW_LSTM.get_global_model().to(device)\n","    model.load_state_dict(torch.load(os.path.join(weights_dir, f'{material}.ckpt'), map_location=device)) # Load trained material model from .ckpt file\n","\n","    num_samples = min(magData.b.shape[0], max_samples)  # Limits number of samples from validation dataset being used\n","\n","    x_data = np.zeros([num_samples, magData.b.shape[1], 3])\n","    x_data[:, :, 0] = magData.b[:num_samples]\n","    x_data[:, :, 1] = magData.freq[:num_samples]\n","    x_data[:, :, 2] = magData.temp[:num_samples]\n","\n","    y_data = magData.loss[:num_samples]\n","\n","    # Now we can pass a batch of sequences through the model\n","    inputs = torch.tensor(x_data, dtype=torch.float32)\n","    outputs = model(inputs)\n","    total_params = sum(p.numel() for p in model.parameters())\n","\n","    print('Data size ', magData.b.shape[0])\n","    print('model parameters: ', total_params)\n","\n","    # get model performance\n","    pred = outputs.detach().numpy() # Get loss prediction\n","    real = y_data # Actual losses\n","\n","    std_loss = linear_std.linear_std()\n","    std_loss.load(os.path.join(training_data_dir, material, 'std_loss.stdd'))\n","\n","    pred = std_loss.unstd(pred)\n","    real = std_loss.unstd(real)\n","\n","    relv_error = abs(pred - real) / real # Relative absolute error\n","    mean_relv_error = np.mean(relv_error)\n","    errors = {\n","        'mean_error': mean_relv_error, \n","        '95_percentile_error': np.percentile(relv_error, 95), \n","        '99_percentile_error': np.percentile(relv_error, 99),\n","        'max_error': relv_error.max\n","    }\n","    error_summary.append(errors)\n","\n","    record = {'material': material}\n","    record.update(errors)\n","    data.append(record)\n","\n","    # Plot and save error histogram\n","    MagNet.Mag_plot(material, relv_error, os.path.join(validation_dir, f'{material}.pdf'))\n","\n","dataframe = pd.DataFrame(data) # Create error dataframe of dictionary lists\n","dataframe.to_csv(os.path.join(validation_dir, 'model_errors.csv'), index=False)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
